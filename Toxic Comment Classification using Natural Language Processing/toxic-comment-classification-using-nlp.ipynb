{"cells":[{"metadata":{"_cell_guid":"9c0cc9c2-ba1f-46ad-bec7-47a437e65d16","_uuid":"c1683e580b8e20f8fb94dc46444695d70efdee57"},"cell_type":"markdown","source":"# Toxic Comment Classification using Natural Language Processing"},{"metadata":{},"cell_type":"markdown","source":"# Data Overview\n\nSource - [Toxic Comment Classification Dataset](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)\n\nYou are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:\n\n- toxic\n- severe_toxic\n- obscene\n- threat\n- insult\n- identity_hate\n\nYou must create a model which predicts a probability of each type of toxicity for each comment.\n\nFile descriptions:\n- train.csv - the training set, contains comments with their binary labels\n- test.csv - the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set   contains some comments which are not included in scoring.\n- sample_submission.csv - a sample submission file in the correct format"},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"_cell_guid":"463f0f87-f96f-435c-8f40-d39dfef8dc36","_uuid":"09df08cb0050aa8fd4b5b7bd6606b4d79b7a9d08","trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport keras\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data"},{"metadata":{"_cell_guid":"62c89f51-8315-4f50-97c9-de3539e884a9","_uuid":"447247729764c3579a4a2d6bf69287abff0b9af1","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/toxicity/train.csv')\ntest = pd.read_csv('../input/toxicity/test.csv')","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"2b307015-414b-466e-9a80-72b9f1b61df8","scrolled":true,"_uuid":"9a4a51116882f985fb362263a1346bf1883a3f04","trusted":true},"cell_type":"code","source":"train.head()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \n0             0        0       0       0              0  \n1             0        0       0       0              0  \n2             0        0       0       0              0  \n3             0        0       0       0              0  \n4             0        0       0       0              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Data Pre-Processing"},{"metadata":{"_cell_guid":"eda2a4d3-c4d4-4d09-a9f3-d3087b0ef96f","scrolled":true,"_uuid":"f5c25b08f4b5504ab984fc8a7b3748f8cef42440","trusted":true},"cell_type":"code","source":"print(train.isnull().any().sum())\nprint(test.isnull().any().sum())","execution_count":10,"outputs":[{"output_type":"stream","text":"0\n0\n","name":"stdout"}]},{"metadata":{"_cell_guid":"4bf0415e-6dcc-40c1-8d04-96b1ac95eb27","_uuid":"548389f5096016fcb405fc73d5c21390d6c72d83","trusted":true},"cell_type":"code","source":"class_list = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[class_list].values\ntrain_sentences = train[\"comment_text\"]\ntest_sentences = test[\"comment_text\"]","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"0d373763-f5c6-4d28-a1d7-f5030d47320f","_uuid":"8d814a2dadca32a8810dca02c7d10dff38a84660","trusted":true},"cell_type":"code","source":"max_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_sentences))\ntrain_tokenized = tokenizer.texts_to_sequences(train_sentences)\ntest_tokenized = tokenizer.texts_to_sequences(test_sentences)","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"d5eca775-e326-421c-a90b-2fc69f14020e","_uuid":"77cb38be6af621207460fa657df85bc70535a21c","trusted":true},"cell_type":"code","source":"max_length = 200\nX_train = pad_sequences(train_tokenized, maxlen=max_length)\nX_test = pad_sequences(test_tokenized, maxlen=max_length)","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"_cell_guid":"f833649b-7a62-4480-9ee9-193128eefb9b","_uuid":"a8578e2b581989b196b3f8296995c3241f32f1d5","trusted":true},"cell_type":"code","source":"inp = Input(shape=(max_length, ))","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"23281f14-130c-41d3-98cc-49283ff27177","_uuid":"9bfea70ef8209f178c9974c61bef16e7ab1bf020","trusted":true},"cell_type":"code","source":"embed_size = 128\nx = Embedding(max_features, embed_size)(inp)\nx = LSTM(60, return_sequences=True,name='lstm_layer')(x)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.1)(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation=\"sigmoid\")(x)","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"8e1fc640-385d-40be-b32c-5703c2635a78","_uuid":"d152b94cc890b512ba6afd9b2fdb0a14c8ea76b4","trusted":true},"cell_type":"code","source":"final_model = Model(inputs=inp, outputs=x)\nfinal_model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"8ae6cadb-0543-4fee-9dba-1eb99987c441","_uuid":"1c26824601d297f618bea32e1e08c3af799fc5a4","trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 2\nfinal_model.fit(X_train,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)","execution_count":18,"outputs":[{"output_type":"stream","text":"Epoch 1/2\n4488/4488 [==============================] - 157s 35ms/step - loss: 0.0680 - accuracy: 0.9595 - val_loss: 0.0484 - val_accuracy: 0.9940\nEpoch 2/2\n4488/4488 [==============================] - 159s 35ms/step - loss: 0.0447 - accuracy: 0.9836 - val_loss: 0.0469 - val_accuracy: 0.9940\n","name":"stdout"},{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f93cc69dd50>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Results"},{"metadata":{"_cell_guid":"0192471f-516d-4f3d-9ce0-784da0ca59ab","_uuid":"5da468e7b497b736510f0521863bc696809e134b","trusted":true},"cell_type":"code","source":"final_model.summary()","execution_count":19,"outputs":[{"output_type":"stream","text":"Model: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 200)]             0         \n_________________________________________________________________\nembedding (Embedding)        (None, 200, 128)          2560000   \n_________________________________________________________________\nlstm_layer (LSTM)            (None, 200, 60)           45360     \n_________________________________________________________________\nglobal_max_pooling1d (Global (None, 60)                0         \n_________________________________________________________________\ndropout (Dropout)            (None, 60)                0         \n_________________________________________________________________\ndense (Dense)                (None, 50)                3050      \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 50)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 6)                 306       \n=================================================================\nTotal params: 2,608,716\nTrainable params: 2,608,716\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = final_model.predict(X_test)\n\nsample_submission = pd.read_csv(\"../input/sample-toxic/sample_submission.csv\")\n\nsample_submission[class_list] = y_test\n\nsample_submission.to_csv(\"toxicity.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}